{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'einops-030:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1046169%2F1760030%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240501%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240501T113246Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0cc20821194d250c6d6ccfb0e1329563bf8d2d00556aee2d97d3ed06e4cbba16bbf175ae2a34934c43e9a70a34a4a67df08affb457ce3f11c65d2d0f3c995ea39d9f4b6736498025d17b1c20b53e14035771090422b7e5c912123f66bf24c90d6df6063361fc7309bc24b44da46f41858d87ab478e24c8653b9e91a1cf371d8907cba1261b72c9c139a868d2bf85071c05e5263ddd748f9e72ad11bec630804ecad785e13d54adc7e3fa0208ca0d14b40715ef1abe46021bbc641573902c748761fba6460a22244f358b06880376e46788758c9bcf3119275a24f6714a3814d84495ddeaaf9c3c63bf2cf16e6519be5b272ce6b341376cfca5cb2ee24cfa0a19,efficientnetpyttorch3d:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1466252%2F2423144%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240501%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240501T113246Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2be668e6548d773ee9c51a7492fd8656c58303505503101c906691b7a4f15be1e48f4213c5d9694ea2b62fec520a94e6418d2f5683f3783da0d9e347bc19b1c92a5f363be2e2429f3c9c686d90898012fee6653d0434045c761e81a2dbeaff7efd9b8ad84f326b0854928899c6c28d1453471ad97a62c27762fcce83f80848375736f2aa863ec3a71202f0f63a55bdfbfbaef692339bacc4c72ce7dc61e30c9c7a3f2ec58dd2a1e160d2af30ec4babb5a51b79369406ea876173a67ba04cbc21070e27d8342964133d15f8823a88011932f769727fed07856d23777112e8ee9a6442f02e7eed94fca836d6009759a880c53459a6965f1208f8f878b47542c810'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "_mwksOC-7d7E"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use stacked images (3D) and Vit model\n",
        "\n",
        "Refernece:\n",
        "\n",
        "- https://www.kaggle.com/rluethy/efficientnet3d-with-one-mri-type\n",
        "- https://github.com/lucidrains/vit-pytorch\n",
        "\n",
        "\n",
        "#### Many thanks to @ROLAND LUETHY\n",
        "* I use @ROLAND LUETHY great notebook, and change the efficient3D model to VIT 3D model, just want to try Vit on 3D\n",
        "* Training log, please check Version1\n",
        "    "
      ],
      "metadata": {
        "id": "lN7XKFzZ7d7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import glob\n",
        "import random\n",
        "import collections\n",
        "import time\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils import data as torch_data\n",
        "from sklearn import model_selection as sk_model_selection\n",
        "from torch.nn import functional as torch_functional\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "papermill": {
          "duration": 1.048295,
          "end_time": "2021-07-14T20:26:46.309722",
          "exception": false,
          "start_time": "2021-07-14T20:26:45.261427",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-07T03:38:34.239112Z",
          "iopub.execute_input": "2021-10-07T03:38:34.239471Z",
          "iopub.status.idle": "2021-10-07T03:38:36.953557Z",
          "shell.execute_reply.started": "2021-10-07T03:38:34.239393Z",
          "shell.execute_reply": "2021-10-07T03:38:36.952714Z"
        },
        "trusted": true,
        "id": "Fzo7h1QW7d7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(\"../input/rsna-miccai-brain-tumor-radiogenomic-classification\"):\n",
        "    data_directory = '../input/rsna-miccai-brain-tumor-radiogenomic-classification'\n",
        "    pytorch3dpath = \"../input/efficientnetpyttorch3d/EfficientNet-PyTorch-3D\"\n",
        "else:\n",
        "    data_directory = '/media/roland/data/kaggle/rsna-miccai-brain-tumor-radiogenomic-classification'\n",
        "    pytorch3dpath = \"EfficientNet-PyTorch-3D\"\n",
        "\n",
        "mri_types = ['FLAIR','T1w','T1wCE','T2w']\n",
        "SIZE = 256\n",
        "NUM_IMAGES = 64\n",
        "\n",
        "sys.path.append(pytorch3dpath)\n",
        "from efficientnet_pytorch_3d import EfficientNet3D"
      ],
      "metadata": {
        "lines_to_end_of_cell_marker": 2,
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.05565,
          "end_time": "2021-07-14T20:26:46.486521",
          "exception": false,
          "start_time": "2021-07-14T20:26:46.430871",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-07T03:38:40.463261Z",
          "iopub.execute_input": "2021-10-07T03:38:40.46359Z",
          "iopub.status.idle": "2021-10-07T03:38:40.49785Z",
          "shell.execute_reply.started": "2021-10-07T03:38:40.46356Z",
          "shell.execute_reply": "2021-10-07T03:38:40.49685Z"
        },
        "trusted": true,
        "id": "WI-ih0iU7d7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to load images"
      ],
      "metadata": {
        "id": "yyeoJnT47d7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dicom_image(path, img_size=SIZE, voi_lut=True, rotate=0):\n",
        "    dicom = pydicom.read_file(path)\n",
        "    data = dicom.pixel_array\n",
        "    if voi_lut:\n",
        "        data = apply_voi_lut(dicom.pixel_array, dicom)\n",
        "    else:\n",
        "        data = dicom.pixel_array\n",
        "\n",
        "    if rotate > 0:\n",
        "        rot_choices = [0, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE, cv2.ROTATE_180]\n",
        "        data = cv2.rotate(data, rot_choices[rotate])\n",
        "\n",
        "    data = cv2.resize(data, (img_size, img_size))\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_dicom_images_3d(scan_id, num_imgs=NUM_IMAGES, img_size=SIZE, mri_type=\"FLAIR\", split=\"train\", rotate=0):\n",
        "\n",
        "    files = sorted(glob.glob(f\"{data_directory}/{split}/{scan_id}/{mri_type}/*.dcm\"),\n",
        "               key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n",
        "\n",
        "    middle = len(files)//2\n",
        "    num_imgs2 = num_imgs//2\n",
        "    p1 = max(0, middle - num_imgs2)\n",
        "    p2 = min(len(files), middle + num_imgs2)\n",
        "    img3d = np.stack([load_dicom_image(f, rotate=rotate) for f in files[p1:p2]]).T\n",
        "    if img3d.shape[-1] < num_imgs:\n",
        "        n_zero = np.zeros((img_size, img_size, num_imgs - img3d.shape[-1]))\n",
        "        img3d = np.concatenate((img3d,  n_zero), axis = -1)\n",
        "\n",
        "    if np.min(img3d) < np.max(img3d):\n",
        "        img3d = img3d - np.min(img3d)\n",
        "        img3d = img3d / np.max(img3d)\n",
        "\n",
        "    return np.expand_dims(img3d,0)\n",
        "\n",
        "a = load_dicom_images_3d(\"00000\")\n",
        "print(a.shape)\n",
        "print(np.min(a), np.max(a), np.mean(a), np.median(a))"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.035761,
          "end_time": "2021-07-14T20:26:46.726756",
          "exception": false,
          "start_time": "2021-07-14T20:26:46.690995",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-07T03:38:40.983171Z",
          "iopub.execute_input": "2021-10-07T03:38:40.983501Z",
          "iopub.status.idle": "2021-10-07T03:38:41.969288Z",
          "shell.execute_reply.started": "2021-10-07T03:38:40.98347Z",
          "shell.execute_reply": "2021-10-07T03:38:41.96779Z"
        },
        "trusted": true,
        "id": "C5kjKMau7d7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed(12)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.668331,
          "end_time": "2021-07-14T20:27:48.114522",
          "exception": false,
          "start_time": "2021-07-14T20:27:47.446191",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-07T03:38:41.970892Z",
          "iopub.execute_input": "2021-10-07T03:38:41.971283Z",
          "iopub.status.idle": "2021-10-07T03:38:42.036373Z",
          "shell.execute_reply.started": "2021-10-07T03:38:41.971244Z",
          "shell.execute_reply": "2021-10-07T03:38:42.035541Z"
        },
        "trusted": true,
        "id": "BK72E_2P7d7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train / test splits"
      ],
      "metadata": {
        "id": "WBLNx55w7d7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(f\"{data_directory}/train_labels.csv\")\n",
        "display(train_df)\n",
        "\n",
        "df_train, df_valid = sk_model_selection.train_test_split(\n",
        "    train_df,\n",
        "    test_size=0.2,\n",
        "    random_state=12,\n",
        "    stratify=train_df[\"MGMT_value\"],\n",
        ")\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.633753,
          "end_time": "2021-07-14T20:27:49.350524",
          "exception": false,
          "start_time": "2021-07-14T20:27:48.716771",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-07T03:38:42.037901Z",
          "iopub.execute_input": "2021-10-07T03:38:42.038213Z",
          "iopub.status.idle": "2021-10-07T03:38:42.075219Z",
          "shell.execute_reply.started": "2021-10-07T03:38:42.038187Z",
          "shell.execute_reply": "2021-10-07T03:38:42.074408Z"
        },
        "trusted": true,
        "id": "FikEBCfC7d7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.tail()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T03:38:42.0766Z",
          "iopub.execute_input": "2021-10-07T03:38:42.076955Z",
          "iopub.status.idle": "2021-10-07T03:38:42.086872Z",
          "shell.execute_reply.started": "2021-10-07T03:38:42.07692Z",
          "shell.execute_reply": "2021-10-07T03:38:42.085681Z"
        },
        "trusted": true,
        "id": "aSb1kuus7d7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model and training classes"
      ],
      "metadata": {
        "id": "GhjIoucb7d7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch_data.Dataset):\n",
        "    def __init__(self, paths, targets=None, mri_type=None, label_smoothing=0.01, split=\"train\", augment=False):\n",
        "        self.paths = paths\n",
        "        self.targets = targets\n",
        "        self.mri_type = mri_type\n",
        "        self.label_smoothing = label_smoothing\n",
        "        self.split = split\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        scan_id = self.paths[index]\n",
        "        if self.targets is None:\n",
        "            data = load_dicom_images_3d(str(scan_id).zfill(5), mri_type=self.mri_type[index], split=self.split)\n",
        "        else:\n",
        "            if self.augment:\n",
        "                rotation = np.random.randint(0,4)\n",
        "            else:\n",
        "                rotation = 0\n",
        "\n",
        "            data = load_dicom_images_3d(str(scan_id).zfill(5), mri_type=self.mri_type[index], split=\"train\", rotate=rotation)\n",
        "\n",
        "        if self.targets is None:\n",
        "            return {\"X\": torch.tensor(data).float(), \"id\": scan_id}\n",
        "        else:\n",
        "            y = torch.tensor(abs(self.targets[index]-self.label_smoothing), dtype=torch.float)\n",
        "            return {\"X\": torch.tensor(data).float(), \"y\": y}\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.634322,
          "end_time": "2021-07-14T20:27:50.594701",
          "exception": false,
          "start_time": "2021-07-14T20:27:49.960379",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-07T03:38:42.555109Z",
          "iopub.execute_input": "2021-10-07T03:38:42.555454Z",
          "iopub.status.idle": "2021-10-07T03:38:42.565665Z",
          "shell.execute_reply.started": "2021-10-07T03:38:42.555423Z",
          "shell.execute_reply": "2021-10-07T03:38:42.564492Z"
        },
        "trusted": true,
        "id": "nqrmlq6K7d7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ../input/einops-030/einops-0.3.0-py2.py3-none-any.whl"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T03:38:42.898083Z",
          "iopub.execute_input": "2021-10-07T03:38:42.898417Z",
          "iopub.status.idle": "2021-10-07T03:39:10.587342Z",
          "shell.execute_reply.started": "2021-10-07T03:38:42.898383Z",
          "shell.execute_reply": "2021-10-07T03:39:10.58639Z"
        },
        "trusted": true,
        "id": "R9AO1sML7d7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "# helpers\n",
        "\n",
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "# classes\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
        "\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\n",
        "        attn = self.attend(dots)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        mlp_dim = 2048\n",
        "        for _ in range(depth):\n",
        "            #print (dim, mlp_dim)\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
        "            ]))\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x\n",
        "\n",
        "# class ViT(nn.Module):\n",
        "#     def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "#         super().__init__()\n",
        "#         image_height, image_width = pair(image_size)\n",
        "#         patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "#         assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "#         num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "#         patch_dim = channels * patch_height * patch_width\n",
        "#         assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "#         self.to_patch_embedding = nn.Sequential(\n",
        "#             Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
        "#             nn.Linear(patch_dim, dim),\n",
        "#         )\n",
        "\n",
        "#         self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "#         self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "#         self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "#         self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "#         self.pool = pool\n",
        "#         self.to_latent = nn.Identity()\n",
        "\n",
        "#         self.mlp_head = nn.Sequential(\n",
        "#             nn.LayerNorm(dim),\n",
        "#             nn.Linear(dim, num_classes)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, img):\n",
        "#         x = self.to_patch_embedding(img)\n",
        "#         b, n, _ = x.shape\n",
        "\n",
        "#         cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
        "#         x = torch.cat((cls_tokens, x), dim=1)\n",
        "#         x += self.pos_embedding[:, :(n + 1)]\n",
        "#         x = self.dropout(x)\n",
        "\n",
        "#         x = self.transformer(x)\n",
        "\n",
        "#         x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "#         x = self.to_latent(x)\n",
        "#         return self.mlp_head(x)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T03:39:10.590925Z",
          "iopub.execute_input": "2021-10-07T03:39:10.591186Z",
          "iopub.status.idle": "2021-10-07T03:39:10.617393Z",
          "shell.execute_reply.started": "2021-10-07T03:39:10.591158Z",
          "shell.execute_reply": "2021-10-07T03:39:10.616638Z"
        },
        "trusted": true,
        "id": "QW2zB3GL7d7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vit Model change to 3D\n",
        "\n",
        "reference:\n",
        "https://github.com/lucidrains/vit-pytorch"
      ],
      "metadata": {
        "id": "nR9B9Vfs7d7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dropout = 0., emb_dropout = 0.):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
        "        num_patches = (image_size // patch_size) *(image_size // patch_size)* 2\n",
        "        patch_dim = channels * patch_size ** 3\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "        #print (mlp_dim)\n",
        "        self.transformer = Transformer(dim, depth, heads, mlp_dim, dropout)\n",
        "        #print (dim)\n",
        "        self.to_cls_token = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_dim, num_classes),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, img, mask = None):\n",
        "        p = self.patch_size\n",
        "        #print (img.shape)\n",
        "        x = rearrange(img, 'b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1 = p, p2 = p, p3 = p)\n",
        "        #print (x.shape)\n",
        "        x = self.patch_to_embedding(x)\n",
        "        #print (x.shape)\n",
        "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
        "        #print (cls_tokens.shape)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        #print (x.shape)\n",
        "        #print (self.pos_embedding.shape)\n",
        "        x += self.pos_embedding\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x = self.to_cls_token(x[:, 0])\n",
        "        return self.mlp_head(x)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T03:39:10.620667Z",
          "iopub.execute_input": "2021-10-07T03:39:10.620921Z",
          "iopub.status.idle": "2021-10-07T03:39:10.633853Z",
          "shell.execute_reply.started": "2021-10-07T03:39:10.620898Z",
          "shell.execute_reply": "2021-10-07T03:39:10.632939Z"
        },
        "trusted": true,
        "id": "Oul08x5U7d7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Model(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.net = EfficientNet3D.from_name(\"efficientnet-b0\", override_params={'num_classes': 2}, in_channels=1)\n",
        "#         n_features = self.net._fc.in_features\n",
        "#         self.net._fc = nn.Linear(in_features=n_features, out_features=1, bias=True)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.net(x)\n",
        "#         return out\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.825458,
          "end_time": "2021-07-14T20:27:55.604161",
          "exception": false,
          "start_time": "2021-07-14T20:27:54.778703",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-07T03:39:10.63526Z",
          "iopub.execute_input": "2021-10-07T03:39:10.635618Z",
          "iopub.status.idle": "2021-10-07T03:39:10.643934Z",
          "shell.execute_reply.started": "2021-10-07T03:39:10.635571Z",
          "shell.execute_reply": "2021-10-07T03:39:10.642989Z"
        },
        "trusted": true,
        "id": "ybIATgf97d7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        device,\n",
        "        optimizer,\n",
        "        criterion\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "\n",
        "        self.best_valid_score = np.inf\n",
        "        self.n_patience = 0\n",
        "        self.lastmodel = None\n",
        "\n",
        "    def fit(self, epochs, train_loader, valid_loader, save_path, patience):\n",
        "        for n_epoch in range(1, epochs + 1):\n",
        "            self.info_message(\"EPOCH: {}\", n_epoch)\n",
        "\n",
        "            train_loss, train_time = self.train_epoch(train_loader)\n",
        "            valid_loss, valid_auc, valid_time = self.valid_epoch(valid_loader)\n",
        "\n",
        "            self.info_message(\n",
        "                \"[Epoch Train: {}] loss: {:.4f}, time: {:.2f} s            \",\n",
        "                n_epoch, train_loss, train_time\n",
        "            )\n",
        "\n",
        "            self.info_message(\n",
        "                \"[Epoch Valid: {}] loss: {:.4f}, auc: {:.4f}, time: {:.2f} s\",\n",
        "                n_epoch, valid_loss, valid_auc, valid_time\n",
        "            )\n",
        "\n",
        "            # if True:\n",
        "            #if self.best_valid_score < valid_auc:\n",
        "            if self.best_valid_score > valid_loss:\n",
        "                self.save_model(n_epoch, save_path, valid_loss, valid_auc)\n",
        "                self.info_message(\n",
        "                     \"auc improved from {:.4f} to {:.4f}. Saved model to '{}'\",\n",
        "                    self.best_valid_score, valid_loss, self.lastmodel\n",
        "                )\n",
        "                self.best_valid_score = valid_loss\n",
        "                self.n_patience = 0\n",
        "            else:\n",
        "                self.n_patience += 1\n",
        "\n",
        "            if self.n_patience >= patience:\n",
        "                self.info_message(\"\\nValid auc didn't improve last {} epochs.\", patience)\n",
        "                break\n",
        "\n",
        "    def train_epoch(self, train_loader):\n",
        "        self.model.train()\n",
        "        t = time.time()\n",
        "        sum_loss = 0\n",
        "\n",
        "        for step, batch in enumerate(train_loader, 1):\n",
        "            X = batch[\"X\"].to(self.device)\n",
        "            targets = batch[\"y\"].to(self.device)\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(X).squeeze(1)\n",
        "\n",
        "            loss = self.criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "\n",
        "            sum_loss += loss.detach().item()\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            message = 'Train Step {}/{}, train_loss: {:.4f}'\n",
        "            self.info_message(message, step, len(train_loader), sum_loss/step, end=\"\\r\")\n",
        "\n",
        "        return sum_loss/len(train_loader), int(time.time() - t)\n",
        "\n",
        "    def valid_epoch(self, valid_loader):\n",
        "        self.model.eval()\n",
        "        t = time.time()\n",
        "        sum_loss = 0\n",
        "        y_all = []\n",
        "        outputs_all = []\n",
        "\n",
        "        for step, batch in enumerate(valid_loader, 1):\n",
        "            with torch.no_grad():\n",
        "                X = batch[\"X\"].to(self.device)\n",
        "                targets = batch[\"y\"].to(self.device)\n",
        "\n",
        "                outputs = self.model(X).squeeze(1)\n",
        "                loss = self.criterion(outputs, targets)\n",
        "\n",
        "                sum_loss += loss.detach().item()\n",
        "                y_all.extend(batch[\"y\"].tolist())\n",
        "                outputs_all.extend(torch.sigmoid(outputs).tolist())\n",
        "\n",
        "            message = 'Valid Step {}/{}, valid_loss: {:.4f}'\n",
        "            self.info_message(message, step, len(valid_loader), sum_loss/step, end=\"\\r\")\n",
        "\n",
        "        y_all = [1 if x > 0.5 else 0 for x in y_all]\n",
        "        auc = roc_auc_score(y_all, outputs_all)\n",
        "\n",
        "        return sum_loss/len(valid_loader), auc, int(time.time() - t)\n",
        "\n",
        "    def save_model(self, n_epoch, save_path, loss, auc):\n",
        "        self.lastmodel = f\"{save_path}-best.pth\"\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model_state_dict\": self.model.state_dict(),\n",
        "                \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
        "                \"best_valid_score\": self.best_valid_score,\n",
        "                \"n_epoch\": n_epoch,\n",
        "            },\n",
        "            self.lastmodel,\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def info_message(message, *args, end=\"\\n\"):\n",
        "        print(message.format(*args), end=end)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.637077,
          "end_time": "2021-07-14T20:27:58.09407",
          "exception": false,
          "start_time": "2021-07-14T20:27:57.456993",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-07T03:39:10.645341Z",
          "iopub.execute_input": "2021-10-07T03:39:10.645687Z",
          "iopub.status.idle": "2021-10-07T03:39:10.667507Z",
          "shell.execute_reply.started": "2021-10-07T03:39:10.645654Z",
          "shell.execute_reply": "2021-10-07T03:39:10.666277Z"
        },
        "trusted": true,
        "id": "USlFgIJp7d7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train models"
      ],
      "metadata": {
        "id": "hPfSD8MZ7d7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train_mri_type(df_train, df_valid, mri_type):\n",
        "    if mri_type==\"all\":\n",
        "        train_list = []\n",
        "        valid_list = []\n",
        "        for mri_type in mri_types:\n",
        "            df_train.loc[:,\"MRI_Type\"] = mri_type\n",
        "            train_list.append(df_train.copy())\n",
        "            df_valid.loc[:,\"MRI_Type\"] = mri_type\n",
        "            valid_list.append(df_valid.copy())\n",
        "\n",
        "        df_train = pd.concat(train_list)\n",
        "        df_valid = pd.concat(valid_list)\n",
        "    else:\n",
        "        df_train.loc[:,\"MRI_Type\"] = mri_type\n",
        "        df_valid.loc[:,\"MRI_Type\"] = mri_type\n",
        "\n",
        "    print(df_train.shape, df_valid.shape)\n",
        "    display(df_train.head())\n",
        "\n",
        "    train_data_retriever = Dataset(\n",
        "        df_train[\"BraTS21ID\"].values,\n",
        "        df_train[\"MGMT_value\"].values,\n",
        "        df_train[\"MRI_Type\"].values,\n",
        "        augment=True\n",
        "    )\n",
        "\n",
        "    valid_data_retriever = Dataset(\n",
        "        df_valid[\"BraTS21ID\"].values,\n",
        "        df_valid[\"MGMT_value\"].values,\n",
        "        df_valid[\"MRI_Type\"].values\n",
        "    )\n",
        "\n",
        "    train_loader = torch_data.DataLoader(\n",
        "        train_data_retriever,\n",
        "        batch_size=4,\n",
        "        shuffle=True,\n",
        "        num_workers=8,pin_memory = True\n",
        "    )\n",
        "\n",
        "    valid_loader = torch_data.DataLoader(\n",
        "        valid_data_retriever,\n",
        "        batch_size=4,\n",
        "        shuffle=False,\n",
        "        num_workers=8,pin_memory = True\n",
        "    )\n",
        "\n",
        "    model = Model(\n",
        "        image_size = 256,\n",
        "        patch_size = 32,\n",
        "        num_classes = 1,\n",
        "        dim = 1024,\n",
        "        depth = 2,\n",
        "        heads = 16,\n",
        "        mlp_dim = 2048,\n",
        "        channels = 1,\n",
        "        dropout = 0.1,\n",
        "        emb_dropout = 0.1\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    #checkpoint = torch.load(\"best-model-all-auc0.555.pth\")\n",
        "    #model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    #print(model)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    #optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    criterion = torch_functional.binary_cross_entropy_with_logits\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model,\n",
        "        device,\n",
        "        optimizer,\n",
        "        criterion\n",
        "    )\n",
        "\n",
        "    history = trainer.fit(\n",
        "        10,\n",
        "        train_loader,\n",
        "        valid_loader,\n",
        "        f\"{mri_type}\",\n",
        "        10,\n",
        "    )\n",
        "\n",
        "    return trainer.lastmodel\n",
        "\n",
        "modelfiles = None\n",
        "\n",
        "if not modelfiles:\n",
        "    modelfiles = [train_mri_type(df_train, df_valid, m) for m in mri_types]\n",
        "    print(modelfiles)"
      ],
      "metadata": {
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 447.387602,
          "end_time": "2021-07-14T20:35:26.110421",
          "exception": false,
          "start_time": "2021-07-14T20:27:58.722819",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-07T03:39:10.668854Z",
          "iopub.execute_input": "2021-10-07T03:39:10.66921Z",
          "iopub.status.idle": "2021-10-07T03:51:53.156535Z",
          "shell.execute_reply.started": "2021-10-07T03:39:10.669175Z",
          "shell.execute_reply": "2021-10-07T03:51:53.154774Z"
        },
        "trusted": true,
        "id": "MO5lOIhw7d7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict function"
      ],
      "metadata": {
        "id": "qnG8GBcY7d7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(modelfile, df, mri_type, split):\n",
        "    print(\"Predict:\", modelfile, mri_type, df.shape)\n",
        "    df.loc[:,\"MRI_Type\"] = mri_type\n",
        "    data_retriever = Dataset(\n",
        "        df.index.values,\n",
        "        mri_type=df[\"MRI_Type\"].values,\n",
        "        split=split\n",
        "    )\n",
        "\n",
        "    data_loader = torch_data.DataLoader(\n",
        "        data_retriever,\n",
        "        batch_size=4,\n",
        "        shuffle=False,\n",
        "        num_workers=8,\n",
        "    )\n",
        "\n",
        "    model = Model(\n",
        "        image_size = 256,\n",
        "        patch_size = 32,\n",
        "        num_classes = 1,\n",
        "        dim = 1024,\n",
        "        depth = 2,\n",
        "        heads = 16,\n",
        "        mlp_dim = 2048,\n",
        "        channels = 1,\n",
        "        dropout = 0.1,\n",
        "        emb_dropout = 0.1\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    checkpoint = torch.load(modelfile)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    model.eval()\n",
        "\n",
        "    y_pred = []\n",
        "    ids = []\n",
        "\n",
        "    for e, batch in enumerate(data_loader,1):\n",
        "        print(f\"{e}/{len(data_loader)}\", end=\"\\r\")\n",
        "        with torch.no_grad():\n",
        "            tmp_pred = torch.sigmoid(model(batch[\"X\"].to(device))).cpu().numpy().squeeze()\n",
        "            if tmp_pred.size == 1:\n",
        "                y_pred.append(tmp_pred)\n",
        "            else:\n",
        "                y_pred.extend(tmp_pred.tolist())\n",
        "            ids.extend(batch[\"id\"].numpy().tolist())\n",
        "\n",
        "    preddf = pd.DataFrame({\"BraTS21ID\": ids, \"MGMT_value\": y_pred})\n",
        "    preddf = preddf.set_index(\"BraTS21ID\")\n",
        "    return preddf"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.990911,
          "end_time": "2021-07-14T20:35:30.482254",
          "exception": false,
          "start_time": "2021-07-14T20:35:29.491343",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-07T03:51:53.160459Z",
          "iopub.execute_input": "2021-10-07T03:51:53.160753Z",
          "iopub.status.idle": "2021-10-07T03:51:53.174723Z",
          "shell.execute_reply.started": "2021-10-07T03:51:53.16072Z",
          "shell.execute_reply": "2021-10-07T03:51:53.173689Z"
        },
        "trusted": true,
        "id": "k2wgjnlK7d7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble for validation"
      ],
      "metadata": {
        "id": "ktu9Y16-7d7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid = df_valid.set_index(\"BraTS21ID\")\n",
        "df_valid[\"MGMT_pred\"] = 0\n",
        "for m, mtype in zip(modelfiles,  mri_types):\n",
        "    pred = predict(m, df_valid, mtype, \"train\")\n",
        "    df_valid[\"MGMT_pred\"] += pred[\"MGMT_value\"]\n",
        "df_valid[\"MGMT_pred\"] /= len(modelfiles)\n",
        "auc = roc_auc_score(df_valid[\"MGMT_value\"], df_valid[\"MGMT_pred\"])\n",
        "print(f\"Validation ensemble AUC: {auc:.4f}\")\n",
        "sns.displot(df_valid[\"MGMT_pred\"])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T03:57:11.579517Z",
          "iopub.execute_input": "2021-10-07T03:57:11.579895Z",
          "iopub.status.idle": "2021-10-07T04:02:30.732944Z",
          "shell.execute_reply.started": "2021-10-07T03:57:11.579862Z",
          "shell.execute_reply": "2021-10-07T04:02:30.731984Z"
        },
        "trusted": true,
        "id": "uHnh9vtj7d7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble for submission"
      ],
      "metadata": {
        "id": "w11e4MWo7d7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv(f\"{data_directory}/sample_submission.csv\", index_col=\"BraTS21ID\")\n",
        "\n",
        "submission[\"MGMT_value\"] = 0\n",
        "for m, mtype in zip(modelfiles, mri_types):\n",
        "    pred = predict(m, submission, mtype, split=\"test\")\n",
        "    submission[\"MGMT_value\"] += pred[\"MGMT_value\"]\n",
        "\n",
        "submission[\"MGMT_value\"] /= len(modelfiles)\n",
        "submission[\"MGMT_value\"].to_csv(\"submission.csv\")"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.990911,
          "end_time": "2021-07-14T20:35:30.482254",
          "exception": false,
          "start_time": "2021-07-14T20:35:29.491343",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-07T04:08:20.512117Z",
          "iopub.execute_input": "2021-10-07T04:08:20.512568Z",
          "iopub.status.idle": "2021-10-07T04:13:00.70838Z",
          "shell.execute_reply.started": "2021-10-07T04:08:20.512512Z",
          "shell.execute_reply": "2021-10-07T04:13:00.707462Z"
        },
        "trusted": true,
        "id": "kaE6qMMY7d7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.972383,
          "end_time": "2021-07-14T20:35:41.939464",
          "exception": false,
          "start_time": "2021-07-14T20:35:40.967081",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-07T04:13:00.710179Z",
          "iopub.execute_input": "2021-10-07T04:13:00.710523Z",
          "iopub.status.idle": "2021-10-07T04:13:00.726317Z",
          "shell.execute_reply.started": "2021-10-07T04:13:00.710485Z",
          "shell.execute_reply": "2021-10-07T04:13:00.725102Z"
        },
        "trusted": true,
        "id": "CyGy8ZC37d7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(submission[\"MGMT_value\"])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T04:13:00.728449Z",
          "iopub.execute_input": "2021-10-07T04:13:00.728989Z",
          "iopub.status.idle": "2021-10-07T04:13:01.024336Z",
          "shell.execute_reply.started": "2021-10-07T04:13:00.728884Z",
          "shell.execute_reply": "2021-10-07T04:13:01.023521Z"
        },
        "trusted": true,
        "id": "l0LEY0SS7d7p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}